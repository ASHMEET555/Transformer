# Transformer from Scratch: Bilingual Language Translator ğŸ‡¬ğŸ‡§â¡ï¸ğŸ‡®ğŸ‡¹

A **ground-up implementation of the Transformer architecture** as proposed in  
**_â€œAttention Is All You Needâ€ (Vaswani et al.)**, built entirely from scratch using **PyTorch**.

This project performs **Neural Machine Translation (NMT)** for **English â†’ Italian** using the **OPUS Books dataset**, with a strong emphasis on mathematical correctness, modular design, and interpretability.

---

## ğŸš€ Highlights

- ğŸ”§ Full Transformer implementation (no `nn.Transformer`)
- ğŸŒ English â†’ Italian translation
- ğŸ§  Multi-Head Self-Attention & Cross-Attention
- ğŸ“ Sinusoidal Positional Encoding
- ğŸ”„ Encoderâ€“Decoder architecture (6 layers each)
- ğŸ§ª BLEU, WER, and CER evaluation
- ğŸ“Š TensorBoard monitoring
- ğŸ’¾ Automatic checkpointing & resume support
- ğŸ‘ï¸ Attention weight visualization

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ config.py                 # Configuration parameters & paths
â”œâ”€â”€ dataset.py                # Dataset processing & masking logic
â”œâ”€â”€ model.py                  # Transformer architecture (Encoder, Decoder, MHA)
â”œâ”€â”€ train.py                  # Training loop, validation & greedy decoding
â”œâ”€â”€ tokenizer_en.json         # Pre-trained English WordLevel tokenizer
â”œâ”€â”€ tokenizer_it.json         # Pre-trained Italian WordLevel tokenizer
â”œâ”€â”€ attention_visual.ipynb    # Attention weight visualization
â”œâ”€â”€ inference.ipynb           # Interactive translation testing
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ opus_books_weights/       # Model checkpoints (auto-generated)

ğŸ—ï¸ Architecture Deep Dive
1ï¸âƒ£ Transformer Model (model.py)

Multi-Head Attention (MHA)
Implements scaled dot-product attention across multiple heads to capture diverse linguistic relationships.

Positional Encoding
Fixed sinusoidal positional encodings to inject sequence order information.

Feed-Forward Network (FFN)
Position-wise fully connected layers with ReLU activation.

Residual Connections & Layer Normalization
Standard Add & Norm blocks for stable deep training.

Encoderâ€“Decoder Stack

Encoder: 6 stacked layers

Decoder: 6 stacked layers

Cross-attention between source and target sequences

2ï¸âƒ£ Data Pipeline (dataset.py)

BilingualDataset

Converts sentence pairs into tokenized tensors

Pads/truncates to fixed sequence length

Causal Masking

Prevents the decoder from attending to future tokens

Preserves autoregressive generation

Special Token Handling

[SOS], [EOS], [PAD] handled automatically

âš™ï¸ Configuration (config.py)

Key hyperparameters (easily adjustable):

Parameter	Value
Batch Size	32
Sequence Length	128
d_model	512
Learning Rate	1e-4
Label Smoothing	0.1
Encoder Layers	6
Decoder Layers	6
ğŸ§ª Training & Evaluation
ğŸ“¦ Prerequisites

Python 3.10+

PyTorch

Hugging Face datasets & tokenizers

torchmetrics

tqdm

TensorBoard

Install dependencies:

pip install torch datasets tokenizers torchmetrics tqdm tensorboard
â–¶ï¸ Training

Start training from scratch or resume from the latest checkpoint:

python train.py

The script automatically detects:

âœ… CUDA (NVIDIA GPUs)

âœ… MPS (Apple Silicon)

âœ… CPU fallback

ğŸ“Š Monitoring & Metrics

TensorBoard Integration

Training & validation loss

Evaluation metrics per epoch

Automated Validation Metrics

BLEU Score

Word Error Rate (WER)

Character Error Rate (CER)

ğŸ‘ï¸ Visualization & Inference

attention_visual.ipynb

Visualize attention maps across heads and layers

inference.ipynb

Interactive notebook for real-time translation testing

ğŸ’¾ Checkpointing

Saves:

Model weights

Optimizer state

Training epoch

Enables seamless training resume

ğŸ“š Dataset

OPUS Books Dataset

Clean, parallel Englishâ€“Italian sentence pairs

Ideal for sentence-level translation tasks

ğŸ§  Learning Objectives

This project is ideal if you want to:

Understand Transformers at a mathematical & implementation level

Build NMT systems without high-level abstractions

Explore attention mechanisms visually

Strengthen PyTorch and NLP fundamentals

ğŸ“Œ Future Improvements

Beam search decoding

Byte-Pair Encoding (BPE)

Transformer variants (Pre-LN, RoPE, etc.)

Mixed precision training

Multi-GPU training support

ğŸ“„ References

Vaswani et al., Attention Is All You Need, 2017

OPUS: Open Parallel Corpus

PyTorch Documentation

â­ Acknowledgements

Inspired by the original Transformer paper and modern NLP research.
Built with a focus on clarity, correctness, and learning.
