# Transformer from Scratch: Bilingual Language Translator ğŸ‡¬ğŸ‡§â¡ï¸ğŸ‡®ğŸ‡¹

A **ground-up implementation of the Transformer architecture** as proposed in  
**_â€œAttention Is All You Needâ€ (Vaswani et al.)**, built entirely from scratch using **PyTorch**.

This project performs **Neural Machine Translation (NMT)** for **English â†’ Italian** using the **OPUS Books dataset**, with a strong emphasis on mathematical correctness, modular design, and interpretability.

---

## ğŸš€ Highlights

- ğŸ”§ Full Transformer implementation (no `nn.Transformer`)
- ğŸŒ English â†’ Italian translation
- ğŸ§  Multi-Head Self-Attention & Cross-Attention
- ğŸ“ Sinusoidal Positional Encoding
- ğŸ”„ Encoderâ€“Decoder architecture (6 layers each)
- ğŸ§ª BLEU, WER, and CER evaluation
- ğŸ“Š TensorBoard monitoring
- ğŸ’¾ Automatic checkpointing & resume support
- ğŸ‘ï¸ Attention weight visualization

---

## ğŸ“‚ Project Structure

```plaintext
.
â”œâ”€â”€ config.py                 # Configuration parameters & paths
â”œâ”€â”€ dataset.py                # Dataset processing & masking logic
â”œâ”€â”€ model.py                  # Transformer architecture (Encoder, Decoder, MHA)
â”œâ”€â”€ train.py                  # Training loop, validation & greedy decoding
â”œâ”€â”€ tokenizer_en.json         # Pre-trained English WordLevel tokenizer
â”œâ”€â”€ tokenizer_it.json         # Pre-trained Italian WordLevel tokenizer
â”œâ”€â”€ attention_visual.ipynb    # Attention weight visualization
â”œâ”€â”€ inference.ipynb           # Interactive translation testing
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ opus_books_weights/       # Model checkpoints (auto-generated)
```
---

## ğŸ—ï¸ Architecture Deep Dive

### 1ï¸âƒ£ Transformer Model (`model.py`)

#### ğŸ”¹ Multi-Head Attention (MHA)
Implements **scaled dot-product attention** across multiple heads to capture diverse syntactic and semantic relationships in language.

#### ğŸ”¹ Positional Encoding
Uses **fixed sinusoidal positional encodings** to inject word order information without introducing additional learned parameters.

#### ğŸ”¹ Feed-Forward Network (FFN)
Position-wise fully connected layers with **ReLU activation**, applied independently to each token representation.

#### ğŸ”¹ Residual Connections & Layer Normalization
Standard **Add & Norm** blocks ensure stable gradient flow and efficient deep training.

#### ğŸ”¹ Encoderâ€“Decoder Stack
- **Encoder:** 6 stacked layers for source sentence encoding  
- **Decoder:** 6 stacked layers for autoregressive target generation  
- **Cross-Attention:** Enables the decoder to attend over encoded source representations  

---

### 2ï¸âƒ£ Data Pipeline (`dataset.py`)

#### ğŸ”¹ BilingualDataset
- Converts raw Englishâ€“Italian sentence pairs into tokenized tensors  
- Pads or truncates sequences to a fixed maximum length  

#### ğŸ”¹ Causal Masking
- Prevents the decoder from attending to future tokens  
- Preserves the autoregressive decoding property  

#### ğŸ”¹ Special Token Handling
Automatically manages:
- `[SOS]` â€” Start of sentence  
- `[EOS]` â€” End of sentence  
- `[PAD]` â€” Padding token  

---

## âš™ï¸ Configuration (`config.py`)

Key hyperparameters (fully configurable):

| Parameter         | Value |
|-------------------|-------|
| Batch Size        | 32    |
| Sequence Length   | 128   |
| d_model           | 512   |
| Learning Rate     | 1e-4  |
| Label Smoothing   | 0.1   |
| Encoder Layers    | 6     |
| Decoder Layers    | 6     |

---

## ğŸ§ª Training & Evaluation

### ğŸ“¦ Prerequisites
- Python **3.10+**
- PyTorch
- Hugging Face `datasets` & `tokenizers`
- `torchmetrics`
- `tqdm`
- TensorBoard

Install dependencies:


â–¶ï¸ Training

Start training from scratch or resume from the latest checkpoint:

python train.py

The training script automatically detects and utilizes:

CUDA (NVIDIA GPUs)

MPS (Apple Silicon)

CPU fallback

ğŸ“Š Monitoring & Metrics
ğŸ”¹ TensorBoard Integration

Training loss

Validation loss

Evaluation metrics per epoch

ğŸ”¹ Evaluation Metrics

BLEU Score

Word Error Rate (WER)

Character Error Rate (CER)

ğŸ‘ï¸ Visualization & Inference
ğŸ”¹ attention_visual.ipynb

Visualize attention weights across layers and heads for interpretability.

ğŸ”¹ inference.ipynb

Interactive notebook for real-time translation testing.

ğŸ’¾ Checkpointing

The training pipeline automatically saves:

Model weights

Optimizer state

Current training epoch

This enables seamless resumption of training.

ğŸ“š Dataset

OPUS Books Dataset

Clean, parallel Englishâ€“Italian sentence pairs

Suitable for sentence-level neural machine translation

ğŸ§  Learning Objectives

This project is designed to help you:

Understand Transformers at a mathematical and implementation level

Build NMT systems without relying on high-level abstractions

Explore attention mechanisms visually

Strengthen PyTorch and NLP fundamentals

ğŸ“Œ Future Improvements

Beam search decoding

Byte-Pair Encoding (BPE)

Transformer variants (Pre-LN, RoPE)

Mixed precision training

Multi-GPU / Distributed training

ğŸ“„ References

Vaswani et al., Attention Is All You Need, 2017

OPUS: Open Parallel Corpus

PyTorch Documentation

â­ Acknowledgements

Inspired by the original Transformer paper and modern NLP research.
Built with a focus on clarity, correctness, and learning.

If you find this project useful, consider starring â­ the repository!
